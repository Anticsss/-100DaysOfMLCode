# 100_Days_Of_ML_Code
## Day 0: December 04, 2018

**Today's Progress:** Setup the system.

1. Enrolled in [Machine Learning course](https://www.coursera.org/learn/machine-learning/home/info) by Stanford University taught by Andrew NG on coursera.

2. Start following [Learn_Machine_Learning_in_3_Months](https://github.com/llSourcell/Learn_Machine_Learning_in_3_Months) by @llSourcell

**Thoughts:** Hope this will help me to do Machine Learning in more effective way and help me to win kaggle competition.


## Day 1: December 06, 2018

**Today's Progress:** Completed week1 of [Machine Learning course](https://www.coursera.org/learn/machine-learning/home/info) by Stanford University taught by Andrew NG on coursera.

1. Learnt about what is Machine Learning
2. What is Supervised and Unsupervised Learning
3. Cost function, Gradient Descent
4. Univariate Linear Regression(ULR) 
5. Reviewed some concepts of Linear Algebra.

**Thoughts:** Understanding how model is represented and how to minimize cost(theta value) function and converge to best fit using gradient descent and normal equation mathematically as well as by coding is actually fun.

## Day 2: December 10, 2018

**Today's Progress:** Completed week2 of [Machine Learning course](https://www.coursera.org/learn/machine-learning/home/info) by Stanford University taught by Andrew NG on coursera.

Submitted the week2 assignment, got 100/100 score :)

1. Learnt Gradient Descent for multiple variables
2. How to do feature scaling 
3. What is Learning rate, Why it is important for better convergence and how to adjust it
4. Polynomial regression

**Thoughts:** Feature scaling and Learning rate is the key to best fit sometimes but analyzing your data also most important

## Day 3,4,5: December 14, 15, 16 2018

**Progress:** learnt how NLTK-3.4 language module works and how to use and apply different preprocessing, models and smoothing techniques to predict next word.

1. Learnt how to prepare data, train model and predicting next word along with score
2. Smoothing algorithms for language modeling.

**Thoughts:** It is somewhat easy to predict next word based on previously known context using language models but is more difficult to predict on domain specific data with lots of grammatic mistakes.
